# rep

A **selfâ€‘hosted playground for Large Language Model (LLM) experimentation**. The repository bundles a minimal Goâ€‘based backend (`gollmâ€‘mini`), a React/TypeScript frontâ€‘end (`gollmâ€‘ui`), helper services for Hugging Face inference (`hfâ€‘api`), and Dockerâ€‘based operational tooling.

> **Why "rep"?**  The name comes from *Rapid Experiment Platform* â€” a place to *rep* ideas quickly, compare models, and capture results.

---

## âœ¨ Key Features

| Area                          | Highlights                                                                                                          |
| ----------------------------- | ------------------------------------------------------------------------------------------------------------------- |
| **Multiâ€‘Provider**            | Plugâ€‘andâ€‘play adapters for Ollama (local), Huggingâ€¯Face endpoints, and OpenAIâ€‘compatible APIs.                      |
| **Prompt & Model Comparison** | Send the same prompt to several models sideâ€‘byâ€‘side, then store graded results for later analysis.                  |
| **Session Memory**            | Perâ€‘conversation context with automatic persistence (PostgreSQL).                                                   |
| **Streaming SSE**             | Tokenâ€‘level streaming responses to the UI for lowâ€‘latency chat.                                                     |
| **Scoring & Analytics**       | Builtâ€‘in rubric/LLMâ€‘based scoring; results are written to the database for offline evaluation.                      |
| **Oneâ€‘Command Deployment**    | `docker compose up` spins up backend, UI, database, Ollama and optional observability stack (Prometheusâ€¯+â€¯Grafana). |

---

## ğŸ—‚ï¸ Repository Layout

```
rep/
â”‚
â”œâ”€ gollm-mini/        # Go backend (REST + SSE)
â”œâ”€ gollm-ui/          # React + Vite frontâ€‘end
â”œâ”€ hf-api/            # Lightweight FastAPI proxy for HuggingFace Inference Endpoints
â”œâ”€ ops/               # Kubernetes & Grafana dashboards, CI scripts, etc.
â”‚
â”œâ”€ docker-compose.yml           # Dev stack (Postgres, Ollama, backend, UI)
â””â”€ docker-compose.observ.yml    # Optional observability addâ€‘ons
```

---

## ğŸš€ QuickÂ Start (Docker)

```bash
# 1. Clone
$ git clone https://github.com/xxxiyxh/rep.git && cd rep

# 2. (Optional) adjust .env files for API keys, ports, etc.

# 3. Launch everything
$ docker compose up -d

# 4. Open the UI
Visit http://localhost:5173  (default Vite dev port)
```

Services started:

| Container                  | Port        | Purpose                                        |
| -------------------------- | ----------- | ---------------------------------------------- |
| `postgres`                 | 5432        | session & scoring storage                      |
| `ollama`                   | 11434       | local LLMs (e.g. llama3)                       |
| `gollm-mini`               | 8080        | REST/SSE API                                   |
| `gollm-ui`                 | 5173        | web client                                     |
| `prometheus` / `grafana`\* | 9090 / 3000 | metrics & dashboards (*observability profile*) |

---

## ğŸ› ï¸ Local Development

### Backend

```bash
cd gollm-mini
# Requires GoÂ 1.22+
go run ./cmd/server -config config.yaml
```

* Hotâ€‘reload is available via `air` (see `.air.toml`).
* Configuration is read from `config.yaml` **plus** envâ€‘vars â€” keep secrets out of source.

### Frontâ€‘end

```bash
cd gollm-ui
# Requires NodeÂ 20+
pnpm install
pnpm dev   # Vite liveâ€‘reload
```

UI is autoâ€‘configured to proxy API calls to `gollm-mini`.

---

## ğŸ”Œ Supported Providers

| Provider              | Models Tested                | Notes                           |
| --------------------- | ---------------------------- | ------------------------------- |
| **Ollama**            | `llama3:8b`, `mistral:7b`, â€¦ | Runs locally inside Docker.     |
| **Huggingâ€¯Face**      | Any Inference Endpoint       | Forwarded via `hf-api` proxy.   |
| **OpenAIâ€‘Compatible** | GPTâ€‘4o, GPTâ€‘3.5â€‘turbo        | Supply `OPENAI_API_KEY` in env. |

Adding a new provider only takes \~20Â LOC â€” implement the `Provider` interface inside `gollm-mini/internal/providers`.

---

## ğŸ“Š Scoring & Result Persistence

Each prompt/response pair is stored with:

* **raw text**
* **model metadata** (provider, latency, cost if available)
* **scores** â€” numeric rubric or LLMâ€‘grader output

This enables offline analysis, A/B testing and dashboarding (Grafana dashboards are provided under `ops/grafana/`).

---

## ğŸ—ï¸ Roadmap

* [ ] **Function calling** / tools API support
* [ ] **Roleâ€‘play agent templates**
* [ ] **WebSockets** alternative to SSE
* [ ] **Fineâ€‘tuning helper scripts**
* [ ] **Helm chart** for cluster deployments

Feel free to open an issue or PR if youâ€™d like to help!

---

## ğŸ¤ Contributing

1. Fork the project & create a feature branch.
2. Follow the linting presets (`golangci-lint`, `eslint`, `prettier`).
3. Add tests for new behaviour.
4. Submit a Pull Request.

All contributors must sign the **DCO** (see `.github/CONTRIBUTING.md`).

